---
title: "Quantifying the Quality of Weight Lifting Exercises With Sensor Data"
author: "Hsing Liu"
date: "September 13, 2014"
output:
  html_document:
    keep_md: true
---

<!--
To knit this RMD file, run:
knit2html("project_writeup.Rmd");browseURL("project_writeup.html")
-->

### Project Description

The goal of this project is to classify the manner in which barbell lifts are performed.  The supplied data contains sensor measurements from six subjects each performing barbell lifts in five different ways, one correctly (labeled classe="A") and four incorrectly (B~E).  The complete description of the study is available at <http://groupware.les.inf.puc-rio.br/har>, and the data set is [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

A machine learning model is trained using the above data set, and applied to a separate test set of 20 cases, which can be found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

### Feature Selection and Preprocessing

R package `caret` is used for model training, and `ggplot2` is used for exploratory plotting.

```{r, cache=TRUE, results='hide'}
library(caret)
library(ggplot2)
```

First, the data sets are downloaded and placed in R's working directory.  Load the data sets as follows:

```{r, cache=TRUE}
training <- read.csv("pml-training.csv", na.strings=c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA", ""))
dim(training)
dim(testing)
```

By manually examining the data as well as referring to the paper associated with the data source, we can see that the data consists of segments of continuous time series of sensor measurements, divided into around 400 "windows."  At the end of each window, statistical summaries such as min, max, mean and variance are calculated for each measurement over this period.

The first consequence of this data structure is the apparent large portion of NA data in the columns with summary variables.  Otherwise, the data is very clean and well structured.  The second consequence is that we'll be building our prediction model quite differently compared to the original study.  This is because in the original study, these summary variables are the primary features that are used to classify movement.  In our case however, since the testing data set does not provide these summary variables, we'll be ignoring them.

Removing these columns is straight forward because only they contain NA's:

```{r, cache=TRUE}
na_counts <- colSums(is.na(training))
table(na_counts)
train_sel <- training[, na_counts == 0]
colnames(train_sel)
```

A few more columns are of no use to us, such as the timestamp and window data.

```{r, cache=TRUE}
train_sel <- train_sel[, -c(1, 3:7)]        # X, timestamp, window
dim(train_sel)
```

At this point we're left with 54 columns.  We might consider removing user_name also, but it is unclear whether that is appropriate:

```{r, cache=TRUE}
qplot(roll_belt, yaw_belt, data=train_sel, color=user_name)
```

From the plot above it appears that each user might execute the motions quite differently, but it also suggests that the difference between users might already be well-encoded by other data.  We'll keep this column for now.

### Model Fitting Using Random Forest

The first machine learning model fitted was random forest.  It also turned out to have the highest accuracy and was applied to the final test cases.

Using the `caret` package, we can automate the out of sample error estimate by setting the train control parameters.  This is done with `trainControl(method="cv")` which defaults to doing a single iteration of K-fold cross-validation, K=10.

To keep the training time reasonable, the option `tuneLength=1` is included.  By default, caret would automatically select several tuning parameters, repeat model fitting for each, going through K-fold cross-validation every time and select the parameter with the lowest estimated error.  In the case of random forest, this parameter is called `mtry`, which is the number of variables sampled at each split.  Caret normally picks 3 different `mtry` values, but experimentations showed that this parameter tend to not have a significant effect on accuracy, so we'll limit caret to use only one default value.

```{r, cache=TRUE, echo=FALSE}
library(randomForest)
```

```{r, cache=TRUE}
ctrl <- trainControl(method="cv")
fit_rf <- train(classe ~ ., data=train_sel, method="rf",
                trControl=ctrl, tuneLength=1)
```

```{r, cache=TRUE}
fit_rf
fit_rf$finalModel
```

Other methods attempted include basic tree (rpart), SVM and some boosting methods.  While SVM achieved 95% accuracy, the other methods did not produce any reasonable result (perhaps not set up correctly), so they are not included in this writeup.

### Out of Sample Error Estimate

The final random forest is shown to have 500 trees, 7 variables tried at each split, and a remarkable out-of-bag error estimate of 0.28%, using 10-fold cross-validation.

An alternative way to get un unbiased OOB estimate for random forest is to specify `trainControl(method="oob")`, which will run much quicker than K-fold CV.  See [This page](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) for more information.

### Variable Importance

The random forest model contains information on the relevance/importance of each variable, which allows us to revisit the question at the beginning, whether user name should be kept.

```{r, cache=TRUE}
decrease_gini <- importance(fit_rf$finalModel)
decrease_gini[order(-decrease_gini), ,drop=F]
```

This shows that user_name contributes little to the model, and can perhaps be safely ignored.

### Test Cases Prediction

Here the trained model is applied to the test cases.  The test set data first needs to be
pre-processed the same way.

```{r, cache=TRUE}
test_sel <- testing[, na_counts == 0]
test_sel <- test_sel[, -c(1, 3:7)]
predict(fit_rf, test_sel)
```
